import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
import warnings
import os
import sys
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
# è®¾ç½®ç¼–ç ç¯å¢ƒå˜é‡ï¼Œè§£å†³Windowsä¸­æ–‡è·¯å¾„é—®é¢˜
os.environ['PYTHONIOENCODING'] = 'utf-8'
if sys.platform.startswith('win'):
    import locale

    locale.setlocale(locale.LC_ALL, 'C')

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤ºæ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8')


class BearingFaultDiagnosisSystem:
    """
    è½´æ‰¿æ•…éšœè¯Šæ–­åˆ†ç±»ç³»ç»Ÿç±»
    """

    def __init__(self, csv_file_path='bearing_features.csv'):
        """
        åˆå§‹åŒ–è¯Šæ–­ç³»ç»Ÿ

        Parameters:
        csv_file_path (str): CSVæ–‡ä»¶è·¯å¾„
        """
        self.csv_file_path = csv_file_path
        self.data = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.models = {}
        self.best_models = {}
        self.results = {}

    def load_and_prepare_data(self):
        """
        åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
        """
        print("=" * 60)
        print("ç¬¬ä¸€æ­¥: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
        print("=" * 60)

        try:
            # åŠ è½½CSVæ–‡ä»¶
            self.data = pd.read_csv(self.csv_file_path)
            print(f"âœ“ æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {self.csv_file_path}")
            print(f"âœ“ æ•°æ®å½¢çŠ¶: {self.data.shape}")

            # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
            print(f"âœ“ åˆ—å: {list(self.data.columns)}")
            print(f"âœ“ æ ‡ç­¾åˆ†å¸ƒ:")
            print(self.data['Label'].value_counts())

        except FileNotFoundError:
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {self.csv_file_path}")
            return False
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½é”™è¯¯: {e}")
            return False

        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        # ç‰¹å¾X: é™¤äº†Filename, Label, RPMä¹‹å¤–çš„æ‰€æœ‰åˆ—
        feature_columns = [col for col in self.data.columns if col not in ['Filename', 'Label', 'RPM']]
        X = self.data[feature_columns]
        y = self.data['Label']

        print(f"âœ“ ç‰¹å¾æ•°é‡: {X.shape[1]}")
        print(f"âœ“ æ ·æœ¬æ•°é‡: {X.shape[0]}")

        # æ ‡ç­¾ç¼–ç 
        y_encoded = self.label_encoder.fit_transform(y)
        self.label_names = self.label_encoder.classes_
        print(f"âœ“ æ ‡ç­¾ç¼–ç æ˜ å°„: {dict(zip(self.label_names, range(len(self.label_names))))}")

        # æ•°æ®åˆ’åˆ† (80% è®­ç»ƒé›†, 20% æµ‹è¯•é›†)
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y_encoded,
            test_size=0.2,
            random_state=42,
            stratify=y_encoded  # ä¿æŒå„ç±»åˆ«æ¯”ä¾‹
        )

        print(f"âœ“ è®­ç»ƒé›†å¤§å°: {self.X_train.shape}")
        print(f"âœ“ æµ‹è¯•é›†å¤§å°: {self.X_test.shape}")

        # ç‰¹å¾æ ‡å‡†åŒ–
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)

        print("âœ“ ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
        print("âœ“ æ•°æ®é¢„å¤„ç†é˜¶æ®µå®Œæˆ!\n")

        return True

    def define_models_and_parameters(self):
        """
        å®šä¹‰æ¨¡å‹å’Œè¶…å‚æ•°æœç´¢èŒƒå›´
        """
        print("=" * 60)
        print("ç¬¬äºŒæ­¥: æ¨¡å‹å®šä¹‰ä¸è¶…å‚æ•°è®¾ç½®")
        print("=" * 60)

        # å®šä¹‰æ¨¡å‹åŠå…¶è¶…å‚æ•°æœç´¢èŒƒå›´
        self.models = {
            'Logistic Regression': {
                'model': LogisticRegression(random_state=42, max_iter=1000),
                'params': {
                    'C': [0.1, 1, 10, 100],
                    'solver': ['liblinear', 'lbfgs']
                }
            },
            'K-Nearest Neighbors': {
                'model': KNeighborsClassifier(),
                'params': {
                    'n_neighbors': [3, 5, 7, 9, 11],
                    'weights': ['uniform', 'distance'],
                    'metric': ['euclidean', 'manhattan']
                }
            },
            'Random Forest': {
                'model': RandomForestClassifier(random_state=42),
                'params': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [None, 10, 20, 30],
                    'min_samples_split': [2, 5, 10]
                }
            },
            'Support Vector Machine': {
                'model': SVC(random_state=42),
                'params': {
                    'C': [0.1, 1, 10, 100],
                    'gamma': ['scale', 'auto', 0.1, 1],
                    'kernel': ['rbf', 'linear']
                }
            }
        }

        print("âœ“ å·²å®šä¹‰ä»¥ä¸‹æ¨¡å‹:")
        for name in self.models.keys():
            print(f"  - {name}")
        print("âœ“ æ¨¡å‹å®šä¹‰å®Œæˆ!\n")

    def train_and_optimize_models(self):
        """
        è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
        """
        print("=" * 60)
        print("ç¬¬ä¸‰æ­¥: æ¨¡å‹è®­ç»ƒä¸è¶…å‚æ•°ä¼˜åŒ–")
        print("=" * 60)

        for name, config in self.models.items():
            print(f"\nğŸ”„ æ­£åœ¨è®­ç»ƒ {name}...")

            # ä½¿ç”¨GridSearchCVè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
            # æ³¨æ„ï¼šåœ¨Windowsç³»ç»Ÿä¸­ï¼Œå¦‚æœç”¨æˆ·è·¯å¾„åŒ…å«ä¸­æ–‡å­—ç¬¦ï¼Œä½¿ç”¨n_jobs=1é¿å…ç¼–ç é—®é¢˜
            grid_search = GridSearchCV(
                estimator=config['model'],
                param_grid=config['params'],
                cv=5,  # 5æŠ˜äº¤å‰éªŒè¯
                scoring='f1_macro',  # ä½¿ç”¨å®å¹³å‡F1åˆ†æ•°ä½œä¸ºè¯„ä¼°æŒ‡æ ‡
                n_jobs=1,  # é¿å…Windowsä¸­æ–‡è·¯å¾„ç¼–ç é—®é¢˜
                verbose=0
            )

            # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œè®­ç»ƒ
            grid_search.fit(self.X_train_scaled, self.y_train)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            self.best_models[name] = grid_search.best_estimator_

            print(f"âœ“ {name} è®­ç»ƒå®Œæˆ")
            print(f"  æœ€ä½³å‚æ•°: {grid_search.best_params_}")
            print(f"  äº¤å‰éªŒè¯æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}")

        print("\nâœ“ æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ!\n")

    def evaluate_models(self):
        """
        åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ‰€æœ‰æ¨¡å‹
        """
        print("=" * 60)
        print("ç¬¬å››æ­¥: æ¨¡å‹æ€§èƒ½è¯„ä¼°")
        print("=" * 60)

        for name, model in self.best_models.items():
            print(f"\nğŸ“Š è¯„ä¼° {name}:")
            print("-" * 40)

            # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
            y_pred = model.predict(self.X_test_scaled)

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            accuracy = accuracy_score(self.y_test, y_pred)
            f1_macro = f1_score(self.y_test, y_pred, average='macro')
            f1_weighted = f1_score(self.y_test, y_pred, average='weighted')

            # ä¿å­˜ç»“æœ
            self.results[name] = {
                'accuracy': accuracy,
                'f1_macro': f1_macro,
                'f1_weighted': f1_weighted,
                'y_pred': y_pred
            }

            # æ‰“å°åˆ†ç±»æŠ¥å‘Š
            print("åˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(
                self.y_test, y_pred,
                target_names=self.label_names,
                digits=4
            ))

        print("âœ“ æ¨¡å‹è¯„ä¼°å®Œæˆ!\n")

    def create_performance_summary(self):
        """
        åˆ›å»ºæ€§èƒ½æ±‡æ€»è¡¨æ ¼
        """
        print("=" * 60)
        print("ç¬¬äº”æ­¥: æ€§èƒ½æ±‡æ€»")
        print("=" * 60)

        # åˆ›å»ºæ±‡æ€»DataFrame
        summary_data = []
        for name, metrics in self.results.items():
            summary_data.append({
                'æ¨¡å‹': name,
                'å‡†ç¡®ç‡': f"{metrics['accuracy']:.4f}",
                'å®å¹³å‡F1': f"{metrics['f1_macro']:.4f}",
                'åŠ æƒå¹³å‡F1': f"{metrics['f1_weighted']:.4f}"
            })

        summary_df = pd.DataFrame(summary_data)

        # æŒ‰å‡†ç¡®ç‡æ’åº
        summary_df['å‡†ç¡®ç‡_æ•°å€¼'] = summary_df['å‡†ç¡®ç‡'].astype(float)
        summary_df = summary_df.sort_values('å‡†ç¡®ç‡_æ•°å€¼', ascending=False)
        summary_df = summary_df.drop('å‡†ç¡®ç‡_æ•°å€¼', axis=1)

        print("ğŸ“‹ æ¨¡å‹æ€§èƒ½æ±‡æ€»è¡¨:")
        print(summary_df.to_string(index=False))

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model_name = max(self.results.keys(),
                              key=lambda x: self.results[x]['accuracy'])

        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
        print(f"   å‡†ç¡®ç‡: {self.results[best_model_name]['accuracy']:.4f}")
        print(f"   å®å¹³å‡F1: {self.results[best_model_name]['f1_macro']:.4f}")

        return best_model_name, summary_df

    def plot_confusion_matrix(self, best_model_name):
        """
        ç»˜åˆ¶æœ€ä½³æ¨¡å‹çš„æ··æ·†çŸ©é˜µ

        Parameters:
        best_model_name (str): æœ€ä½³æ¨¡å‹åç§°
        """
        print(f"\nğŸ“ˆ ç»˜åˆ¶ {best_model_name} çš„æ··æ·†çŸ©é˜µ...")

        # è·å–é¢„æµ‹ç»“æœ
        y_pred = self.results[best_model_name]['y_pred']

        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.y_test, y_pred)

        # åˆ›å»ºå›¾å½¢
        plt.figure(figsize=(10, 8))

        # ä½¿ç”¨seabornç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(
            cm,
            annot=True,
            fmt='d',
            cmap='Blues',
            xticklabels=self.label_names,
            yticklabels=self.label_names,
            cbar_kws={'label': 'æ ·æœ¬æ•°é‡'}
        )

        plt.title(f'{best_model_name} - æ··æ·†çŸ©é˜µ\nå‡†ç¡®ç‡: {self.results[best_model_name]["accuracy"]:.4f}',
                  fontsize=16, fontweight='bold')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=14)
        plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=14)

        # è°ƒæ•´æ ‡ç­¾å­—ä½“å¤§å°
        plt.xticks(fontsize=12)
        plt.yticks(fontsize=12, rotation=0)

        plt.tight_layout()
        plt.show()

        print("âœ“ æ··æ·†çŸ©é˜µç»˜åˆ¶å®Œæˆ!")

    def run_complete_analysis(self):
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        """
        print("ğŸš€ å¼€å§‹è½´æ‰¿æ•…éšœè¯Šæ–­åˆ†æç³»ç»Ÿ")
        print("=" * 60)

        # æ­¥éª¤1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
        if not self.load_and_prepare_data():
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
            return

        # æ­¥éª¤2: å®šä¹‰æ¨¡å‹å’Œå‚æ•°
        self.define_models_and_parameters()

        # æ­¥éª¤3: è®­ç»ƒå’Œä¼˜åŒ–æ¨¡å‹
        self.train_and_optimize_models()

        # æ­¥éª¤4: è¯„ä¼°æ¨¡å‹
        self.evaluate_models()

        # æ­¥éª¤5: åˆ›å»ºæ€§èƒ½æ±‡æ€»å’Œå¯è§†åŒ–
        best_model_name, summary_df = self.create_performance_summary()
        self.plot_confusion_matrix(best_model_name)

        print("\n" + "=" * 60)
        print("ğŸ‰ è½´æ‰¿æ•…éšœè¯Šæ–­åˆ†æå®Œæˆ!")
        print("=" * 60)

        return {
            'best_model': best_model_name,
            'best_model_object': self.best_models[best_model_name],
            'results': self.results,
            'summary': summary_df,
            'label_encoder': self.label_encoder,
            'scaler': self.scaler
        }


def main():
    """
    ä¸»å‡½æ•° - è¿è¡Œè½´æ‰¿æ•…éšœè¯Šæ–­ç³»ç»Ÿ
    """
    # åˆ›å»ºè¯Šæ–­ç³»ç»Ÿå®ä¾‹
    diagnosis_system = BearingFaultDiagnosisSystem('bearing_features.csv')

    # è¿è¡Œå®Œæ•´åˆ†æ
    results = diagnosis_system.run_complete_analysis()

    return results


if __name__ == "__main__":
    # è¿è¡Œä¸»ç¨‹åº
    final_results = main()